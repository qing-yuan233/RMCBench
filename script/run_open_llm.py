from transformers import AutoTokenizer
import transformers
import torch
import pandas as pd
import time
# from check_refuse import refuse_generate_malicious_code
from datetime import datetime
import json
import re
import os
import argparse

# å…¨å±€å˜é‡

# è¦æµ‹è¯•çš„8ä¸ªå¼€æºæ¨¡å‹çš„ç›®å½•å’Œåå­—
model_info_dict = [
    {
        "mid":0,
        "dir_name":"llama-2-7b-chat-hf",
        "LLM_name":"Llama"
    },
    {
        "mid":1,
        "dir_name":"llama-2-13b-chat-hf",
        "LLM_name":"Llama"
    },
    {
        "mid":2,
        "dir_name":"tulu-2-13b",
        "LLM_name":"Tulu"
    },
    {
        "mid":3,
        "dir_name":"zephyr-7b-beta",
        "LLM_name":"Zephyr"
    },
    {
        "mid":4,
        "dir_name":"mpt-7b-chat",
        "LLM_name":"Mpt"
    },
    {
        "mid":5,
        "dir_name":"vicuna-7b-v1.3",
        "LLM_name":"Vicuna"
    },
    {
        "mid":6,
        "dir_name":"CodeLlama-13b-Instruct-hf",
        "LLM_name":"CodeLlama"
    },
    {
        "mid":7,
        "dir_name":"deepseek-coder-7b-instruct-v1.5",
        "LLM_name":"Deepseek"
    },
    {
        "mid":8,
        "dir_name":"Meta-Llama-3-8B-Instruct",
        "LLM_name":"Llama"
    }
]

model_name = "" # æ¨¡å‹åå­—(å¸¦å‚æ•°)
pipeline = "" # æ¨¡å‹pipline
tokenizer = "" # åˆ†è¯å™¨

# æ ¹æ®ä¸åŒLLMçš„prompt chatæ¨¡æ¿è¿›è¡Œæ ¼å¼åŒ–
def format_prompt(user_message):
    global model_name
    # llamaç³»åˆ—
    if model_name in ["llama-2-7b-chat-hf","llama-2-13b-chat-hf","CodeLlama-13b-Instruct-hf"]:
        new_prompt = f"""<s>[INST] <<SYS>>
You are a helpful assistant.
<</SYS>>

{ user_message } [/INST]
"""
        return new_prompt
    
    # tuluç³»åˆ—
    if model_name == "tulu-2-13b":
        new_prompt = f"""<|user|>
{ user_message }
<|assistant|>
"""
        return  new_prompt

    # zephyr-7b-beta
    if model_name == "zephyr-7b-beta":
        new_prompt = f"""<|system|>
you are a helpful assistant.</s>
<|user|>
{ user_message }</s>
<|assistant|>
"""
        return new_prompt
    
    # mpt-7b-chat
    if model_name == "mpt-7b-chat":
        new_prompt = f"""<|im_start|>user
{ user_message }<|im_end|>
<|im_start|>assistant
"""
        return new_prompt

    if model_name == "vicuna-7b-v1.3":
        new_prompt = f"""USER:
{ user_message }
ASSISTANT:
"""
        return  new_prompt

    if model_name == "deepseek-coder-7b-instruct-v1.5":
        new_prompt = f"""You are an AI programming assistant.
### Instruction:
{ user_message }
### Response:
"""
        return  new_prompt
        
    return user_message

    

# æ¨ç†ï¼Œè·å–LLMå›ç­”
def get_response(prompt):

    global pipeline,tokenizer

    # æ ¹æ®ä¸åŒçš„æ¨¡å‹è¿›è¡Œæ ¼å¼åŒ–
    user_message = prompt
    prompt = format_prompt(prompt)

    # æ£€æŸ¥æ˜¯å¦æ ¼å¼åŒ–ï¼Œæ²¡æœ‰è¿›è¡Œæ ¼å¼åŒ–ï¼Œåˆ™æ²¡æœ‰æ­£ç¡®åŒ¹é…åˆ°æ¨¡å‹
    if user_message == prompt:
        print("ä½ æ‰€ä½¿ç”¨çš„æ¨¡å‹æ²¡æœ‰promptæ¨¡æ¿ï¼")
        sys.exit()
    print("\nå·²å°†ä¸Šè¿°promptæŒ‰ç…§æ¨¡å‹ä¸“å±chatæ¨¡æ¿è¿›è¡Œæ ¼å¼åŒ–")
    
    terminators = tokenizer.eos_token_id
    # llama3,å¤šäº†ä¸€ä¸ªé…ç½®
    
    sequences = pipeline(
        prompt, # å…·ä½“çš„æç¤ºè¯
        do_sample=True,
        top_k=10,
        top_p=0.9,
        temperature = 0.75,
        num_return_sequences=1,
        max_new_tokens=4096, # ç”Ÿæˆçš„æœ€å¤§tokenæ•°
        # max_length = 4096 # æ¨¡å‹æœ€å¤§tokenæ•°
    )

    for seq in sequences:
        # print(f"Result: {seq['generated_text']}")
        context = seq['generated_text'] # å®Œæ•´çš„è¾“å…¥+å›ç­”
        # åªè·å–æ¨¡å‹å›ç­”,ä¸åŒ…å«æé—®
        only_response = context.split(prompt, 1)[-1]
        return only_response


# æ›¿æ¢promptä¸­é’ˆå¯¹chatGPTè®¾è®¡çš„éƒ¨åˆ†,è®²LLMçš„åå­—æ”¹ä¸ºå…·ä½“æµ‹è¯•æ¨¡å‹çš„åå­—
def change_LLM_name_in_prompt(old_prompt,LLM_name):
    if LLM_name == "chatgpt" :
        return old_prompt
    
    result_str = re.sub('chatgpt', LLM_name, old_prompt, flags=re.IGNORECASE)
    return result_str

# ä¸»å‡½æ•°
def main():

    # logo
    print("""
â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—
â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘
â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘
â•šâ•â•     â•šâ•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•â• â•šâ•â•â•â•â•â•â•šâ•â•  â•šâ•â•                                                               
          """)
    
    global model_info_dict,pipeline,tokenizer,model_name
    
    # é€šè¿‡å‘½ä»¤æ¥å—å‚æ•°
    parser = argparse.ArgumentParser(description='é€šè¿‡å‚æ•°æŒ‡å®šæµ‹è¯•çš„æ¨¡å‹ã€ä»¥åŠå®éªŒè½®æ•°')
    parser.add_argument('--run_model_index',type=int,required=True, help='è¿è¡Œæ¨¡å‹çš„ç´¢å¼•')
    parser.add_argument('--run_rounds', type=int,nargs='+',required=True, help='æœ¬æ¬¡å®éªŒæ˜¯ç¬¬å‡ è½®(å¯ä»¥æ˜¯å¤šè½®)ï¼Œæ•°å­—ä»¥ç©ºæ ¼åˆ†å‰²')

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()

    # æ ¹æ®è¾“å…¥å‚æ•°è‡ªåŠ¨ç”Ÿæˆçš„å…³é”®å‚æ•°
    run_model_index = args.run_model_index
    run_rounds = args.run_rounds

    print("ä¸€å…±éœ€è¦è¿›è¡Œçš„è½®æ•°ï¼š",run_rounds)
    for run_round in run_rounds:
        print(f"ç¬¬{run_round}è½®å®éªŒ")

        file_path = f'' # åŒ…å«promptçš„xlxsæ–‡ä»¶
        model_info = model_info_dict[run_model_index]
        model_path = "" # æ¨¡å‹ç›®å½•
        output_file_dir = '' # è¾“å‡ºæ–‡ä»¶è·¯å¾„,éœ€è¦åŒ…å«æœ€åçš„/
        LLM_name = model_info["LLM_name"] # ä¸å¸¦ä»»ä½•å‚æ•°çš„LLMæ¨¡å‹åï¼Œç”¨äºæ›¿æ¢è¶Šç‹±æç¤ºè¯ä¸­çš„æŒ‡ç¤ºå¯¹è±¡

        print("******************************æœ¬æ¬¡è¿è¡Œå…³é”®å‚æ•°:******************************")
        print("model_path:",model_path)
        print("output_file_dir:",output_file_dir)
        print("LLM_name:",LLM_name)
        print("****************************************************************************")

        # æ£€æŸ¥è¾“å‡ºè·¯å¾„
        if os.path.exists(output_file_dir):
            print(f"è¾“å‡ºç›®å½• {output_file_dir} å­˜åœ¨ã€‚")
        else:
            print(f"è¾“å‡ºç›®å½• {output_file_dir} ä¸å­˜åœ¨,è¿›è¡Œåˆ›å»º")
            os.makedirs(output_file_dir)
            print("åˆ›å»ºå®Œæˆï¼")

        model_name = model_path.split('/')[-1] # æ¨¡å‹åå­—å¸¦å‚æ•°

        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨===============================================================
        print("åŠ è½½æ¨¡å‹...")
        tokenizer = AutoTokenizer.from_pretrained(model_path)

        pipeline = transformers.pipeline(
        "text-generation",
        model=model_path,
        torch_dtype=torch.float16,
        device_map="auto",
        )
        print("åŠ è½½å®Œæˆï¼")
        print("ğŸ¤©ğŸ¤©ğŸ¤©ğŸ¤©ğŸ¤© "+model_name+",å¯åŠ¨ï¼ï¼ï¼ğŸ¤©ğŸ¤©ğŸ¤©ğŸ¤©ğŸ¤©")
        # ===============================================================================

        # å¼€å§‹æ—¶é—´
        start_time = time.perf_counter()
        
        # ä»æ–‡ä»¶ä¸­è·å–prompt
        prompt_df = pd.read_excel(file_path)

        # ******************debug æŠ½æ ·æµ‹è¯•**********************************************************************************************************************************************
        # prompt_df = prompt_df.sample(n=10)
        # # é‡ç½®ç´¢å¼•ï¼Œæ–°çš„dataframeç´¢å¼•ä¹Ÿæ˜¯ä»0å¼€å§‹
        # prompt_df = prompt_df.reset_index(drop=True)
        # *****************************************************************************************************************************************************************************

        # åˆå§‹åŒ–åˆ›å»ºåˆ—å­˜å‚¨ç»“æœçš„åˆ—,ä»¥åŠæ˜¯å¦æ‹’ç»å›ç­”
        prompt_df['response'] = ''

        # è®¡ç®—æ ·æœ¬æ•°
        num_rows = len(prompt_df)
        print("total row:", num_rows)

        index = 0  # åˆå§‹åŒ–ç´¢å¼•
        same_error_times=0 # åˆå§‹åŒ–åŒä¸€æ¡æ ·æœ¬å‡ºç°é”™è¯¯çš„æœ€å¤§æ¬¡æ•°
        error_id_list = [] # å‡ºé”™çš„æ ·æœ¬çš„id

        while index < num_rows:
            # if index >=5:
            #     break

            print(f"now is processing the indexï¼š{index}  data=============================")
            
            # æ ¹æ®ç´¢å¼•å–åˆ°è¡Œ
            row = prompt_df.iloc[index]
            print("pid:",row['pid'])

            # è·å–å…·ä½“çš„prompt
            prompt = row['prompt']

            # å¦‚æœä¸ºè¶Šç‹±ï¼Œåœ¨è¿™é‡ŒåŠ¨æ€æ›¿æ¢è¶Šç‹±æç¤ºè¯ä¸­æŒ‡ç¤ºçš„æ¨¡å‹
            if row['category']=="text-to-code" and row["level"]==3:
                prompt = change_LLM_name_in_prompt(prompt,LLM_name)
                prompt_df.at[index, 'prompt'] = prompt # æ›¿æ¢åˆ°ç»“æœä¸­
                print("å®Œæˆè¶Šç‹±è¯æ›¿æ¢")

            print("\nprompt:")
            print(prompt)

            try:
                response = get_response(prompt)
                # å½“æœ¬æ¡æ ·æœ¬æé—®å®Œæˆæ—¶ï¼Œé”™è¯¯æ•°æ¸…é›¶
                same_error_times=0
            except Exception as e:
                # å¢åŠ ç´¯è®¡é”™è¯¯æ•°
                print("å‡ºé”™äº†ï¼")

            # æ‰“å°è¾“å‡ºæ•°æ®
            print("\nLLM:")
            print(response)
            print("\n====================================================================================\n")
            # æ£€æŸ¥æ˜¯å¦æ‹’ç»ç”Ÿæˆ

            # æ— æ•ˆæ•°æ®ï¼Œé€šå¸¸æ¨¡å‹ä¼šä¸åœçš„è¾“å‡ºå¾ˆå¤šç©ºç™½è¡Œï¼Œä¼šå½±å“dfä¿å­˜ä¸ºæ–‡ä»¶ï¼Œå¯¼è‡´å†™å…¥æ•°æ®ä¸ºç©ºï¼Œæ‰€ä»¥ä¸ä¿ç•™å…¶å›ç­”
            response_line = response.count('\n') + 1
            if response_line >=500: # æ•°æ®é›†ä¸­æœ€é•¿çš„ä»£ç æ–‡ä»¶ä¹Ÿæ‰347è¡Œï¼Œæ‰€ä»¥è¾“å‡ºè¡Œæ•°åŸºæœ¬ä¸å¯èƒ½æ¯”è¿™ä¸ªå¤š
                response  = "Long text exception output.Prevent memory overflow and replace it."
                print(response)

            # æ•°æ®æ›´æ–°åˆ°dfä¸­
            prompt_df.at[index, 'response'] = str(response)

            # å¤„ç†æˆåŠŸï¼Œç§»åŠ¨åˆ°ä¸‹ä¸€è¡Œ
            index += 1

        # æ‰€æœ‰promptæ¨ç†å®Œæˆ=======================================================
            
        # æœ€ç»ˆä¿å­˜çš„ç»“æœï¼Œåç¼€ä¸ºdoneï¼
        timestamp = time.strftime("%Y%m%d%H%M%S")
        
        output_file = output_file_dir + f"res_{model_name}_{timestamp}_round{run_round}_done.xlsx"
        print("æ–‡ä»¶ä¿å­˜è·¯å¾„",output_file)

        print("ä¿å­˜ä¸ºjsonå’Œxlxs")
        json_name = output_file_dir + f"res_{model_name}_{timestamp}_round{run_round}_done.json"
        prompt_df.to_json(json_name,orient='records')
        prompt_df.to_excel(output_file, index=False)

        # è®¡ç®—è€—æ—¶
        end_time = time.perf_counter()
        e2e_inference_time = (end_time-start_time)
        print(f"The time consuming is {e2e_inference_time} s")

        # æœ¬æ¬¡è¿è¡Œçš„æ‰€æœ‰ä¿¡æ¯
        run_info = {
            "model_name":model_name,
            "run_time":timestamp,
            "time consuming":f"{e2e_inference_time} s",
            "error_id_list":error_id_list
        }
        # ä¿¡æ¯ä¿å­˜åˆ°æ–‡ä»¶
        with open(output_file_dir + f"run_info_{model_name}_{timestamp}_round{run_round}_done.json", 'w') as f:
            json.dump(run_info, f)

        print("è¿è¡Œå®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶å·²ä¿å­˜åˆ°ç›®å½•ï¼š", output_file_dir)
    
    print("æ‰€æœ‰è½®æ¬¡å®éªŒå·²æ‰§è¡Œå®Œæ¯•ï¼")

if __name__ == "__main__":
    main()

